{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['戓', '炰', '僝', '僽', '忺', '笯', '姱', '胾', '臁', '踧']\n",
      "['噼', '嘭', '瘼', '疐', '拑', '滆', '厳', '颃', '衒', '囟']\n",
      "['瘗', '鹡', '鲅', '偎', '晧', '泐', '遛', '垴', '瞑', '赅']\n",
      "['鸱', '藿', '呦', '拏', '仟', '磴', '胝', '抡', '咭', '蛆']\n",
      "['砺', '盱', '咱', '牍', '砥', '毎', '轲', '怂', '糠', '恿']\n",
      "['筿', '鲱', '颉', '佟', '湳', '歙', '璜', '巍', '嶋', '胪']\n",
      "['洼', '闰', '缮', '诬', '掀', '吟', '洽', '堰', '镍', '桨']\n",
      "['抛', '惧', '奸', '弄', '抄', '肢', '夸', '呎', '棕', '绥']\n",
      "['挑', '篮', '倍', '纵', '慈', '莲', '紫', '塘', '邓', '晶']\n",
      "['拔', '瑞', '照', '征', '望', '述', '依', '预', '注', '督']\n",
      "['在', '一', '年', '的']\n",
      "['盱', '咱', '牍', '砥', '毎', '轲', '怂', '糠', '恿', '苌', '浬', '箍', '韭', '鳝', '郗', '褪', '囤', '闫', '帑', '鲭', '馒', '窍', '锆', '嵯', '澹', '猥', '辎', '瓢', '嘘', '锗']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/home/ljh/CSC/Enhanced_Syllable_Feature/data/counter.json', 'r') as f:\n",
    "    counter = json.load(f)\n",
    "\n",
    "zi_list = sorted(list(counter.keys()), key= lambda x: counter[x])\n",
    "\n",
    "for index in range(0,len(zi_list),len(zi_list) // 10):\n",
    "    print(zi_list[index : index+10])\n",
    "\n",
    "index = int(len(zi_list) * 0.4)\n",
    "print(zi_list[index : index+30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5615\n",
      "fix len 5615\n",
      "avg min/max 119.05574354407837 1 413\n",
      "quantile 0.0 1\n",
      "quantile 0.1 27\n",
      "quantile 0.2 60\n",
      "quantile 0.30000000000000004 81\n",
      "quantile 0.4 96\n",
      "quantile 0.5 114\n",
      "quantile 0.6000000000000001 129\n",
      "quantile 0.7000000000000001 151\n",
      "quantile 0.8 177\n",
      "quantile 0.9 215\n",
      "fix len 4321\n",
      "avg min/max 112.57556121268225 0 366\n",
      "quantile 0.0 0\n",
      "quantile 0.1 46\n",
      "quantile 0.2 65\n",
      "quantile 0.30000000000000004 78\n",
      "quantile 0.4 91\n",
      "quantile 0.5 104\n",
      "quantile 0.6000000000000001 119\n",
      "quantile 0.7000000000000001 138\n",
      "quantile 0.8 160\n",
      "quantile 0.9 192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import opencc\n",
    "import json\n",
    "if sys.path[0] != '/home/ljh/CSC/Enhanced_Syllable_Feature':\n",
    "    sys.path.insert(0,  '/home/ljh/CSC/Enhanced_Syllable_Feature')\n",
    "from datasets.utils import pho_convertor\n",
    "\n",
    "converter = opencc.OpenCC('t2s.json')\n",
    "\n",
    "\n",
    "def traditional_to_simple(text):\n",
    "    text = converter.convert(text)\n",
    "    text = text.replace('著', '着').replace('妳', '你')\n",
    "    return text\n",
    "\n",
    "\n",
    "def _is_chinese_char(cp):\n",
    "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "            (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_chinese_token(token):\n",
    "    if len(token) == 1:\n",
    "        return _is_chinese_char(ord(token))\n",
    "    # if token.startwith('##') and len(token)==3:\n",
    "    #     return  _is_chinese_char(ord(token[2]))\n",
    "    return False\n",
    "def get_confusionset():\n",
    "    with open('/home/ljh/github/CHINESE_GRC_DATA/merge_confusion.json', 'r', encoding='utf-8') as f:\n",
    "        confusionset = json.load(f)\n",
    "    vocab_file = '/home/ljh/model/ChineseBERT-base/vocab.txt'\n",
    "    chinese_list = []\n",
    "    with open(vocab_file, 'r', encoding='utf-8', errors='ignore')as f:\n",
    "        for line in f.readlines():\n",
    "            token = line.strip()\n",
    "            if len(token) == 1 and _is_chinese_char(ord(token)):\n",
    "                chinese_list.append(converter.convert(token))\n",
    "    chinese_list = list(set(chinese_list))\n",
    "    print(len(confusionset))\n",
    "    for key in confusionset:\n",
    "        confusionset[key] = [val for val in confusionset[key] if val in chinese_list]\n",
    "    new_confusion = {}\n",
    "    for key in confusionset:\n",
    "        if key in chinese_list:\n",
    "            new_confusion[key] = confusionset[key]\n",
    "    print('fix len',len(new_confusion))\n",
    "    confusionset = new_confusion\n",
    "    import numpy as np \n",
    "    def statistics(num_list):\n",
    "        print('avg min/max',sum(num_list) / len(num_list), min(num_list), max(num_list))\n",
    "        sorted_list = sorted(num_list)\n",
    "        for quantile in np.arange(0,1,0.1):\n",
    "            print('quantile', quantile, sorted_list[int(len(sorted_list) * quantile)])\n",
    "    statistics([len(val) for _, val in confusionset.items()])\n",
    "\n",
    "    chinese_list = zi_list[int(len(zi_list) * 0.4) : ]\n",
    "    for key in confusionset:\n",
    "        confusionset[key] = [val for val in confusionset[key] if val in chinese_list]\n",
    "    new_confusion = {}\n",
    "    for key in confusionset:\n",
    "        if key in chinese_list:\n",
    "            new_confusion[key] = confusionset[key]\n",
    "    print('fix len',len(new_confusion))\n",
    "    confusionset = new_confusion\n",
    "    statistics([len(val) for _, val in confusionset.items()])\n",
    "    return confusionset\n",
    "\n",
    "\n",
    "confusionset = get_confusionset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wo3'], ['hen3'], ['kuai4'], ['le4'], ['not chinese'], ['not chinese'], ['not chinese'], ['not chinese'], ['not chinese'], ['not chinese'], ['not chinese'], ['xi3', 'xi1', 'chi4'], ['huan1'], ['yin1'], ['yue4'], ['not chinese'], ['cen1'], ['ci1'], ['bu4'], ['qi2'], ['not chinese'], ['ren2'], ['shen1'], ['hao3'], ['chi1']]\n"
     ]
    }
   ],
   "source": [
    "from pypinyin import pinyin, Style\n",
    "sentence = '我很快乐，happy，喜欢音乐,参差不齐，人参好吃'\n",
    "pinyin_list = pinyin(sentence, style=Style.TONE3, heteronym=True, errors=lambda x: [['not chinese'] for _ in x])\n",
    "print(pinyin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 181 91\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "        '/home/ljh/model/ChineseBERT-base/vocab.txt', lowercase = True)\n",
    "output = tokenizer.decode([101, 19476, 2861, 5018, 2697, 2418, 2137, 2526, 2989, 6835, 7390, 3198, 7313, 1359, 1265, 4638, 1770, 1767, 2582, 3416, 4495, 7357, 8020, 2697, 2418, 1139, 8021, 4510, 6605, 511, 3692, 4828, 2697, 2418, 3221, 6387, 1914, 1355, 4510, 3322, 4638, 6817, 868, 1333, 4415, 511, 891, 1963, 8024, 4572, 1779, 3181, 6760, 4638, 3340, 2501, 4828, 7188, 833, 772, 4495, 3198, 5753, 19789, 1767, 8024, 6821, 1348, 833, 4495, 2768, 4510, 1767, 8024, 886, 1765, 6943, 6818, 4638, 7308, 1726, 1750, 9459, 5445, 2697, 2418, 1139, 7441, 3837, 511, 102])\n",
    "print(type(output),len(output),len([101, 19476, 2861, 5018, 2697, 2418, 2137, 2526, 2989, 6835, 7390, 3198, 7313, 1359, 1265, 4638, 1770, 1767, 2582, 3416, 4495, 7357, 8020, 2697, 2418, 1139, 8021, 4510, 6605, 511, 3692, 4828, 2697, 2418, 3221, 6387, 1914, 1355, 4510, 3322, 4638, 6817, 868, 1333, 4415, 511, 891, 1963, 8024, 4572, 1779, 3181, 6760, 4638, 3340, 2501, 4828, 7188, 833, 772, 4495, 3198, 5753, 19789, 1767, 8024, 6821, 1348, 833, 4495, 2768, 4510, 1767, 8024, 886, 1765, 6943, 6818, 4638, 7308, 1726, 1750, 9459, 5445, 2697, 2418, 1139, 7441, 3837, 511, 102]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "file = '/home/ljh/CSC/Enhanced_Syllable_Feature/data/finetun_data/train_all'\n",
    "with open(file, 'r' ,encoding='utf8') as f:\n",
    "    data = list(f.readlines())\n",
    "num = num2 = 0\n",
    "for ex in data:\n",
    "    example = json.loads(ex)\n",
    "    input_ids, pinyin_ids, label,pinyin_label = example['input_ids'], example['pinyin_ids'], example['label'], example['pinyin_label']\n",
    "    example_id,src,tokens_size = example['id'], example['src'], example['tokens_size']\n",
    "    # convert list to tensor\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "    pinyin_ids = torch.LongTensor(pinyin_ids).view(-1)\n",
    "\n",
    "    label = torch.LongTensor(label)\n",
    "    pinyin_label=torch.LongTensor(pinyin_label)\n",
    "    assert input_ids.shape == label.shape\n",
    "    assert input_ids.shape[0] * 8 == pinyin_ids.shape[0]\n",
    "    assert input_ids.shape[0] == pinyin_label.shape[0]\n",
    "    assert input_ids.shape[0] < 512\n",
    "    if input_ids.shape[0]> 256:\n",
    "        num +=1\n",
    "    if input_ids.shape[0]> 192:\n",
    "        num2 +=1\n",
    "print(num, num2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35b97f689774c23c9928ab7d98612524220dc2a0d7f99e1b55b43331667c5c81"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('PYNET': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
